{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math, os\n",
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math,random,re\n",
    "import time\n",
    "#Machine learning/Stats imports \n",
    "from scipy.stats import norm\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "from sklearn.preprocessing import normalize\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points:  50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PABP_YEAST/115-210</td>\n",
       "      <td>qrdpslrkKGSGNIFIKNLHPDIDNKALYDTFSVFGDILSSKIATD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/203-294</td>\n",
       "      <td>..epangsPKFFNVYVKNLPEKYTDDDLKSEFEAFGEITSAVVVKD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/534-625</td>\n",
       "      <td>..epangsPKFFNVYVKNLPEKYTDDDLKSEFEASGEITSAVVVKD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/312-398</td>\n",
       "      <td>........IRGLNLYLKNLDDTIDDERLKELFRPFGTIISCKVMVD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/33-112</td>\n",
       "      <td>........---ASLYVGDLDLSVTEGQLFDLFSQIGPVASVRVCRD...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               name  \\\n",
       "0                                PABP_YEAST/115-210   \n",
       "1  ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/203-294   \n",
       "2  ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/534-625   \n",
       "3  ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/312-398   \n",
       "4   ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/33-112   \n",
       "\n",
       "                                            sequence  \n",
       "0  qrdpslrkKGSGNIFIKNLHPDIDNKALYDTFSVFGDILSSKIATD...  \n",
       "1  ..epangsPKFFNVYVKNLPEKYTDDDLKSEFEAFGEITSAVVVKD...  \n",
       "2  ..epangsPKFFNVYVKNLPEKYTDDDLKSEFEASGEITSAVVVKD...  \n",
       "3  ........IRGLNLYLKNLDDTIDDERLKELFRPFGTIISCKVMVD...  \n",
       "4  ........---ASLYVGDLDLSVTEGQLFDLFSQIGPVASVRVCRD...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Invariants\n",
    "ORDER_KEY=\"XILVAGMFYWEDQNHCRKSTPBZ-\"[::-1]\n",
    "ORDER_LIST=list(ORDER_KEY)\n",
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "from helper_tools import *\n",
    "from helper_tools_for_plotting import *\n",
    "data=pdataframe_from_alignment_file(\"PABP_YEAST_hmmerbit_plmc_n5_m30_f50_t0.2_r115-210_id100_b48.a2m\",50000)\n",
    "print (\"number of data points: \",len(data))\n",
    "data_set_size=len(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "(50000, 1968)\n",
      "number of mutants:  1188\n",
      "1187 1187\n",
      "[('K', 'K'), ('G', 'G'), ('S', 'S'), ('G', 'N'), ('N', 'N'), ('I', 'I'), ('F', 'F'), ('I', 'I'), ('K', 'K'), ('N', 'N')]\n",
      "number of mutants:  36522\n",
      "13876 13876\n",
      "[('G', 'G'), ('K', 'K'), ('S', 'S'), ('K', 'K'), ('G', 'G'), ('F', 'F'), ('G', 'A'), ('F', 'I'), ('V', 'V'), ('H', 'H')]\n"
     ]
    }
   ],
   "source": [
    "indices=index_of_non_lower_case_dot(data.iloc[0][\"sequence\"])\n",
    "data[\"seq\"]=list(map(prune_seq,data[\"sequence\"]))\n",
    "data.head()\n",
    "PRUNED_SEQ_LENGTH=len(data.iloc[0][\"seq\"])\n",
    "with open (\"PABP_YEAST_hmmerbit_t0.2_r50000.reweight\",\"rb\") as to_read:\n",
    "    new_weights=np.load(to_read)\n",
    "\n",
    "#new_weights=reweight_sequences(data[\"seq\"],0.1)\n",
    "len(new_weights),new_weights[:10]\n",
    "#Encode training data in one_hot vectors\n",
    "training_data_one_hot=[]\n",
    "labels=[]\n",
    "for i, row in data.iterrows():\n",
    "    training_data_one_hot.append(translate_string_to_one_hot(row[\"seq\"],ORDER_LIST))\n",
    "print (len(training_data_one_hot))\n",
    "#plt.imshow(training_data_one_hot[0],cmap=\"Greys\")\n",
    "training_data=np.array([np.array(list(sample.flatten())).T for sample in training_data_one_hot])\n",
    "print(training_data.shape)\n",
    "exp_data_full=pd.read_csv(\n",
    "    \"PABP_YEAST_Fields2013-singles.csv\", sep=\";\", comment=\"#\"\n",
    ")\n",
    "print (\"number of mutants: \",len(exp_data_full))\n",
    "exp_data_full.head()\n",
    "exp_data_full.iloc[87]\n",
    "exp_data_full.corr(method=\"spearman\")\n",
    "OFFSET=117\n",
    "#Deciding offset requires investigating the dataset and alignment.\n",
    "exp_data_singles=pd.DataFrame(columns=exp_data_full.columns)\n",
    "#decide starting index depending on how the file is \"headered\"\n",
    "for i,row in exp_data_full[1:].iterrows():\n",
    "        pos=re.split(r'(\\d+)', row.mutant) \n",
    "        if int(pos[1])-OFFSET in indices:\n",
    "            exp_data_singles=exp_data_singles.append(row)\n",
    "exp_data_singles=exp_data_singles.reset_index()\n",
    "target_values_singles=list(exp_data_singles[\"linear\"])\n",
    "exp_data_singles.head(10) \n",
    "mutation_data=[re.split(r'(\\d+)', s) for s in exp_data_singles.mutant]#split the mutant coll e.g.N,126,C\n",
    "wt_sequence=data.iloc[0].seq#wt:KGSGNIFIKNLHPDIDNKALYDTFSVFGDILSS\n",
    "mutants=mutate_single(wt_sequence,mutation_data,offset=0,index=3) #note that you change index to 1\n",
    "\n",
    "#sanity checks\n",
    "print (len(mutants),len(exp_data_singles))\n",
    "#the mutant should be in the correct place\n",
    "print (list(zip(wt_sequence,mutants[3]))[:10])#change the index [3] G to N and pring the first 10 position of mutant\n",
    "#Test data with wt at 0 index\n",
    "one_hot_mutants=[]#encode each matant to one hot \n",
    "mutants_plus=[data.iloc[0][\"seq\"]]+mutants\n",
    "for mutant in mutants_plus:\n",
    "    one_hot_mutants.append(translate_string_to_one_hot(\"\".join(mutant),ORDER_LIST))\n",
    "\n",
    "test_data_plus=np.array([np.array(list(sample.flatten())).T for sample in one_hot_mutants])\n",
    "exp_data_full=pd.read_csv(\n",
    "    \"PABP_YEAST_Fields2013-doubles.csv\", sep=\";\", comment=\"#\"\n",
    ")\n",
    "print (\"number of mutants: \",len(exp_data_full))\n",
    "exp_data_full.head()\n",
    "exp_data_full.iloc[0]\n",
    "exp_data_full.corr(method=\"spearman\")\n",
    "OFFSET=160\n",
    "#Deciding offset requires investigating the dataset and alignment.\n",
    "exp_data_doubles=pd.DataFrame(columns=exp_data_full.columns)\n",
    "#decide starting index depending on how the file is \"headered\"\n",
    "for i,row in exp_data_full[0:].iterrows():\n",
    "        pos=re.split(r'(\\d+)', row.mutant) \n",
    "        if int(pos[1])-OFFSET in indices and int(pos[3])-OFFSET in indices:\n",
    "            exp_data_doubles=exp_data_doubles.append(row)\n",
    "exp_data_doubles=exp_data_doubles.reset_index()\n",
    "exp_data_doubles.head(5)\n",
    "target_values_doubles=list(exp_data_doubles[\"XY_Enrichment_score\"])\n",
    "exp_data_doubles.corr(method=\"spearman\")\n",
    "mutation_data1=[re.split(r'(\\d+)', s.split(\",\")[0]) for s in exp_data_doubles.mutant]\n",
    "mutation_data2=[re.split(r'(\\d+)', s.split(\",\")[1]) for s in exp_data_doubles.mutant]\n",
    "wt_sequence=data.iloc[0].seq#KGSGNIFIKNLHPDIDNKALYDTFSVFGDILSS\n",
    "\n",
    "mutants_double=mutate_double(wt_sequence,mutation_data1,mutation_data2,offset=0,index=46)\n",
    "\n",
    "#sanity checks\n",
    "print (len(mutants_double),len(exp_data_doubles))\n",
    "#the mutant should be in the correct place\n",
    "print (list(zip(wt_sequence,mutants_double[2]))[40:50])\n",
    "#Test data with wt at 0 index\n",
    "one_hot_mutants=[]\n",
    "mutants_plus=[data.iloc[0][\"seq\"]]+mutants_double\n",
    "for mutant in mutants_plus:\n",
    "    one_hot_mutants.append(translate_string_to_one_hot(\"\".join(mutant),ORDER_LIST))\n",
    "\n",
    "test_data_doubles_plus=np.array([np.array(list(sample.flatten())).T for sample in one_hot_mutants])\n",
    "all_test_data=np.vstack([test_data_plus,test_data_doubles_plus[1:]])\n",
    "all_test_data_flattened=np.array([np.array(list(sample.flatten())).T for sample in all_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rho_vs_mutants(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,mutants,test_set_size,aa_size,sequence_size):\n",
    "        self.mutants=mutants\n",
    "        self.sample_size=test_set_size\n",
    "        self.aa_size=aa_size\n",
    "        self.sequence_size=sequence_size\n",
    "        self.scores=[]\n",
    "        self.count_batch=0\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "    #This allows us to track the \"progress\" of the model on different epochs\n",
    "    def on_epoch_end(self,batch,logs):\n",
    "        x_decoded=vae.predict(test_data_plus[0:self.sample_size],batch_size=batch_size)\n",
    "        digit = x_decoded[0].reshape(self.aa_size,self.sequence_size)\n",
    "        digit_wt = normalize(digit,axis=0, norm='l1')\n",
    "        wt_prob=compute_log_probability(digit,digit_wt)\n",
    "        fitnesses=[]\n",
    "        for sample in range(1,self.sample_size):\n",
    "            digit = x_decoded[sample].reshape(self.aa_size,self.sequence_size)\n",
    "            digit = normalize(digit,axis=0, norm='l1')\n",
    "            fitness=compute_log_probability(test_data_plus[sample].reshape(self.aa_size,self.sequence_size),digit)-wt_prob\n",
    "            fitnesses.append(fitness)\n",
    "#Recontruction sample generated from the decoder with test data after each epoch            \n",
    "#Using these reconstructions, compute the (log) probability of a sequence\n",
    "#Once we have computed P(σi) for every sequence σi in our mutant set, \n",
    "#we can compare the rho of each sequence with the experimentally measured function of the sequence.\n",
    "        print (\",\"+str(spearmanr(fitnesses,target_values_singles[:self.sample_size-1])))\n",
    "        #target_values_singles=list(exp_data_singles[\"linear\"])\n",
    "        self.scores.append(spearmanr(fitnesses,target_values_singles[:self.sample_size-1])[0])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.optimizers import Adam\n",
    "opt = Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "batch_size = 20\n",
    "original_dim=len(ORDER_LIST)*PRUNED_SEQ_LENGTH\n",
    "output_dim=len(ORDER_LIST)*PRUNED_SEQ_LENGTH\n",
    "latent_dim = 2\n",
    "intermediate_dim = 250#500\n",
    "nb_epoch = 2\n",
    "epsilon_std = 1.0\n",
    "np.random.seed(42)  \n",
    "#cnn based vae  \n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = tf.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = original_dim * tf.keras.backend.categorical_crossentropy(x,  x_decoded_mean)\n",
    "    kl_loss = - 0.5 * tf.keras.backend.sum(1 + z_log_var - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "#Encoding Layers\n",
    "x = tf.keras.Input(batch_shape=(batch_size, original_dim))\n",
    "h = layers.Dense(intermediate_dim,activation=\"elu\")(x)\n",
    "h= tf.keras.layers.Dropout(0.7)(h)\n",
    "h = layers.Dense(intermediate_dim, activation='elu')(h)\n",
    "h = layers.Dense(intermediate_dim, activation='elu')(h)\n",
    "#Latent layers\n",
    "z_mean=layers.Dense(latent_dim)(h)\n",
    "z_log_var=layers.Dense(latent_dim)(h)\n",
    "z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "#Decoding layers \n",
    "\n",
    "decoder_1= layers.Dense(intermediate_dim, activation='elu')\n",
    "decoder_2=layers.Dense(intermediate_dim, activation='elu')\n",
    "decoder_2d=tf.keras.layers.Dropout(0.7)\n",
    "decoder_3=layers.Dense(intermediate_dim, activation='elu')\n",
    "decoder_out=layers.Dense(output_dim, activation='sigmoid')\n",
    "x_decoded_mean = decoder_out(decoder_3(decoder_2d(decoder_2(decoder_1(z)))))\n",
    "\n",
    "vae = tf.keras.Model(x, x_decoded_mean)\n",
    "\n",
    "#Potentially better results, but requires further hyperparameter tuning\n",
    "#optimizer=keras.optimizers.SGD(lr=0.005, momentum=0.001, decay=0.0, nesterov=False,clipvalue=0.05)\n",
    "vae.compile(optimizer=opt, loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 51s 1ms/step - loss: 916702.2992 - val_loss: 973547.4422\n",
      ",SpearmanrResult(correlation=0.5873628344830507, pvalue=2.8626867977705775e-110)\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 48s 1ms/step - loss: 871235.0018 - val_loss: 968743.8510\n",
      ",SpearmanrResult(correlation=0.6113857361252801, pvalue=8.876069601990554e-122)\n"
     ]
    }
   ],
   "source": [
    "x_train=training_data[:data_set_size] #this needs to be divisible by batch size and less than or equal to dataset size\n",
    "x_train = x_train.astype('float32')\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "spearman_measure=rho_vs_mutants(test_data_plus,batch_size*int(len(test_data_plus)/batch_size),len(ORDER_LIST),PRUNED_SEQ_LENGTH)\n",
    "\n",
    "hist=vae.fit(x_train,x_train ,\n",
    "        shuffle=True,\n",
    "        nb_epoch=nb_epoch,\n",
    "        batch_size=batch_size, #sample_weight=np.array(new_weights),\n",
    "        validation_split=0.1,callbacks=[early_stopping,spearman_measure])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
