{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math, os\n",
    "# Define some handy network layers\n",
    "\n",
    "def lrelu(x, rate=0.1):\n",
    "    return tf.maximum(tf.minimum(x * rate, 0), x)\n",
    "\n",
    "def conv2d_lrelu(inputs, num_outputs, kernel_size, stride):\n",
    "    conv = tf.contrib.layers.convolution2d(inputs, num_outputs, kernel_size, stride, \n",
    "                                           weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           activation_fn=tf.identity)\n",
    "    conv = lrelu(conv)\n",
    "    return conv\n",
    "\n",
    "def conv2d_t_relu(inputs, num_outputs, kernel_size, stride):\n",
    "    conv = tf.contrib.layers.convolution2d_transpose(inputs, num_outputs, kernel_size, stride,\n",
    "                                                     weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                     activation_fn=tf.identity)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    return conv\n",
    "\n",
    "def fc_lrelu(inputs, num_outputs):\n",
    "    fc = tf.contrib.layers.fully_connected(inputs, num_outputs,\n",
    "                                           weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           activation_fn=tf.identity)\n",
    "    fc = lrelu(fc)\n",
    "    return fc\n",
    "\n",
    "def fc_relu(inputs, num_outputs):\n",
    "    fc = tf.contrib.layers.fully_connected(inputs, num_outputs,\n",
    "                                           weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           activation_fn=tf.identity)\n",
    "    fc = tf.nn.relu(fc)\n",
    "    return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder and decoder use the DC-GAN architecture\n",
    "# Define some handy network layers\n",
    "'''\n",
    "tf.reshape(\n",
    "    tensor,\n",
    "    shape,\n",
    "    name=None)\n",
    "shape=tf.placeholder(tf.float32, shape=[None, 227,227,3] )\n",
    "我们经常会这样来feed数据,如果在运行的时候想知道None到底是多少,这时候,只能通过tf.shape(x)[0]这种方式来获得.\n",
    "tensor.get_shape(),返回是一个tuple\n",
    "'''\n",
    "#定义结构:\n",
    "#1：输入是train_x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])，即28*28*1\n",
    "#None指的batch size的大小，所以可以是任何数。\n",
    "#2：encoder的layers包括2个卷积层conv1和conv2+1个fc层fc1\n",
    "#cov1的输入是[None, 28, 28, 1]，根据公式N = (W − F + 2P )/S+1，可以计算得输出是28-4=24/2=12+1=13*13*64=[None,13,13,64]\n",
    "#cov2的输入是[None,13,13,64]，输出是13-4=9/2=4.5，当尺寸不被整除时，卷积向下取整，池化向上取整，因此=4+1=5*5*128=[None,5,5,128]\n",
    "#cov2转换shape为3200[None,3200],None个3200的一维vector\n",
    "#fc1的输入是3200，输出是1024个output，[None,1024]\n",
    "#3：encoder的输出是fc的20个output[None,20]\n",
    "#4.decoder的layer有2个fc层和1个卷积层，输出层是反卷积层\n",
    "#fc1输入是20，输出是1024，[None,1024]\n",
    "#fc2输入是1024，输出是7*7*128=6272,[None,6272]\n",
    "#fc2[None,6272] reshape成[None,7,7,128]\n",
    "#cov1层输入是[None,7,7,128],根据dc-gan的结构,输出是[None,14,14,64]\n",
    "#5.output层输入是[None,14,14,64]，输出是[None,28,28,64]\n",
    "\n",
    "\n",
    "def encoder(x, z_dim):\n",
    "    with tf.variable_scope('encoder'):\n",
    "        conv1 = conv2d_lrelu(x, 64, 4, 2)#(inputs, num_outputs, kernel_size, stride)\n",
    "        #None*13*13*64\n",
    "        conv2 = conv2d_lrelu(conv1, 128, 4, 2)#None*5*5*128\n",
    "        conv2 = tf.reshape(conv2, [-1, np.prod(conv2.get_shape().as_list()[1:])])    \n",
    "        '''\n",
    "        conv2.get_shape().as_list()=[None,5,5,128]\n",
    "         conv2.get_shape().as_list()[1:]=[5,5,128]\n",
    "         np.prod(conv2.get_shape().as_list()[1:])=3200\n",
    "         tf.reshape(-1,3200)=[None,3200]\n",
    "        '''\n",
    "        fc1 = fc_lrelu(conv2, 1024)\n",
    "        return tf.contrib.layers.fully_connected(fc1, z_dim, activation_fn=tf.identity)\n",
    "def decoder(z, reuse=False):\n",
    "    with tf.variable_scope('decoder') as vs:\n",
    "        if reuse:\n",
    "            vs.reuse_variables()\n",
    "        fc1 = fc_relu(z, 1024)\n",
    "        fc2 = fc_relu(fc1, 7*7*128)\n",
    "        fc2 = tf.reshape(fc2, tf.stack([tf.shape(fc2)[0], 7, 7, 128]))\n",
    "        conv1 = conv2d_t_relu(fc2, 64, 4, 2)\n",
    "        output = tf.contrib.layers.convolution2d_transpose(conv1, 1, 4, 2, activation_fn=tf.sigmoid)\n",
    "        return output\n",
    "# Build the computation graph for training\n",
    "z_dim = 20\n",
    "train_x = tf.placeholder(tf.float32, shape=[None, 24, 82, 1])\n",
    "train_z = encoder(train_x, z_dim)\n",
    "train_xr = decoder(train_z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
